{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## MindSpore-BERT-NER\n",
    "### 1. 下载源码和数据至本地容器\n",
    "\n",
    "因为notebook是挂载在obs上，运行的容器实例不能直接读取操作obs上的文件，需下载至容器本地环境中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import moxing as mox\n",
    "mox.file.copy_parallel(src_url=\"obs://nlp-workspace/3.ner/src/\", dst_url='./src/') \n",
    "mox.file.copy_parallel(src_url=\"obs://nlp-workspace/3.ner/data/\", dst_url='./data/')\n",
    "mox.file.copy_parallel(src_url=\"obs://nlp-workspace/3.ner/pre_model/\", dst_url='./pre_model/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2. 导入依赖库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.metrics import classification_report # 需放在前面导入\n",
    "\n",
    "import mindspore.nn as nn\n",
    "from easydict import EasyDict as edict\n",
    "import mindspore.common.dtype as mstype\n",
    "from mindspore import context\n",
    "from mindspore import log as logger\n",
    "from mindspore.common.tensor import Tensor\n",
    "import mindspore.dataset as de\n",
    "from mindspore.ops import operations as P\n",
    "import mindspore.dataset.transforms.c_transforms as C\n",
    "from mindspore.nn.wrap.loss_scale import DynamicLossScaleUpdateCell\n",
    "from mindspore.nn.optim import AdamWeightDecay\n",
    "from mindspore.train.model import Model\n",
    "from mindspore.train.callback import CheckpointConfig, ModelCheckpoint, TimeMonitor, LossMonitor\n",
    "from mindspore.train.serialization import load_checkpoint, load_param_into_net\n",
    "from mindspore.common.initializer import TruncatedNormal\n",
    "\n",
    "from src import tokenization\n",
    "from src.CRF import CRF\n",
    "from src.CRF import postprocess\n",
    "from src.cluener_evaluation import process_one_example_p, label_generation\n",
    "from src.utils import BertLearningRate\n",
    "from src.bert_for_finetune import BertFinetuneCell\n",
    "from src.config import optimizer_cfg\n",
    "from src.bert_model import BertConfig, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "context.set_context(mode=context.GRAPH_MODE, device_target=\"Ascend\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3. 定义参数配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cfg = edict({\n",
    "    'is_train': True,\n",
    "    'num_labels': 41,                 \n",
    "    'schema_file': r'./data/clue_ner/schema.json',\n",
    "    'ckpt_prefix': 'bert-ner-crf',          # 'bert-ner'  'bert-ner-crf'\n",
    "    'train_file': r'./data/clue_ner/train.tf_record', \n",
    "    'eval_file': r'./data/clue_ner/dev.tf_record',\n",
    "    'use_crf': True,         \n",
    "\n",
    "    'epoch_num': 5,\n",
    "    'batch_size': 16,\n",
    "    'ckpt_dir': 'ckpt',\n",
    "    'pre_training_ckpt': './pre_model/bert_base.ckpt',\n",
    "\n",
    "    'finetune_ckpt': './ckpt/bert-ner-crf-5_671.ckpt', \n",
    "    'label2id_file': './data/clue_ner/label2id.json',\n",
    "    'vocab_file': './data/vocab.txt',\n",
    "    'eval_out_file': 'ner_crf_result.txt'      #  ner_result.txt   ner_crf_result.txt\n",
    "})\n",
    "\n",
    "bert_net_cfg = BertConfig(\n",
    "    seq_length=128,\n",
    "    vocab_size=21128,\n",
    "    hidden_size=768,\n",
    "    num_hidden_layers=12,\n",
    "    num_attention_heads=12,\n",
    "    intermediate_size=3072,\n",
    "    hidden_act=\"gelu\",\n",
    "    hidden_dropout_prob=0.1,\n",
    "    attention_probs_dropout_prob=0.1,\n",
    "    max_position_embeddings=512,\n",
    "    type_vocab_size=2,\n",
    "    initializer_range=0.02,\n",
    "    use_relative_positions=False,\n",
    "    dtype=mstype.float32,\n",
    "    compute_type=mstype.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 4. 定义数据集加载函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_dataset(data_file, schema_file, batch_size):\n",
    "    '''\n",
    "    get dataset\n",
    "    '''\n",
    "    ds = de.TFRecordDataset([data_file], schema_file, columns_list=[\"input_ids\", \"input_mask\",\"segment_ids\", \"label_ids\"])\n",
    "    type_cast_op = C.TypeCast(mstype.int32)\n",
    "    ds = ds.map(input_columns=\"segment_ids\", operations=type_cast_op)\n",
    "    ds = ds.map(input_columns=\"input_mask\", operations=type_cast_op)\n",
    "    ds = ds.map(input_columns=\"input_ids\", operations=type_cast_op)\n",
    "    ds = ds.map(input_columns=\"label_ids\", operations=type_cast_op)\n",
    "    \n",
    "    # apply shuffle operation\n",
    "    buffer_size = 960\n",
    "    ds = ds.shuffle(buffer_size=buffer_size)\n",
    "\n",
    "    # apply batch operations\n",
    "    ds = ds.batch(batch_size, drop_remainder=True)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "数据集测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[128], dtype=Int32, value= [ 101, 2218, 3221, 2601, 8013,  122,  126, 3330,  961, 1394, 7183, 4767,  151,  151,  122,  127, 3324,  120,  123, 3647,  120,  122, 1221, 5526, \n",
       " 1164, 3249, 6858, 1276, 6981, 3219, 1921, 4374, 5442, 5783, 5438,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0, \n",
       "    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0, \n",
       "    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0, \n",
       "    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0, \n",
       "    0,    0,    0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(get_dataset(cfg.train_file, cfg.schema_file, batch_size=1).create_dict_iterator())['input_ids'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 5. 定义BertNER模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class BertNER(nn.Cell):\n",
    "    \"\"\"\n",
    "    Train interface for sequence labeling finetuning task.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, batch_size, is_training, num_labels=11, use_crf=False, tag_to_index=None, dropout_prob=0.0,\n",
    "                 use_one_hot_embeddings=False):\n",
    "        super(BertNER, self).__init__()\n",
    "        self.bert = BertModel(config, is_training, use_one_hot_embeddings)\n",
    "        self.cast = P.Cast()\n",
    "        self.weight_init = TruncatedNormal(config.initializer_range)\n",
    "        self.log_softmax = P.LogSoftmax(axis=-1)\n",
    "        self.dtype = config.dtype\n",
    "        self.num_labels = num_labels\n",
    "        self.dense_1 = nn.Dense(config.hidden_size, self.num_labels, weight_init=self.weight_init,\n",
    "                                has_bias=True).to_float(config.compute_type)\n",
    "        self.dropout = nn.Dropout(1 - dropout_prob)\n",
    "        self.reshape = P.Reshape()\n",
    "        self.shape = (-1, config.hidden_size)\n",
    "        self.use_crf = use_crf\n",
    "        self.origin_shape = (batch_size, config.seq_length, self.num_labels)\n",
    "        if use_crf:\n",
    "            if not tag_to_index:\n",
    "                raise Exception(\"The dict for tag-index mapping should be provided for CRF.\")\n",
    "            self.loss = CRF(tag_to_index, batch_size, config.seq_length, is_training)\n",
    "        else:\n",
    "            self.loss = CrossEntropyCalculation(is_training)\n",
    "        self.num_labels = num_labels\n",
    "        self.use_crf = use_crf\n",
    "        \n",
    "    def construct(self, input_ids, input_mask, token_type_id, label_ids):\n",
    "        sequence_output, _, _ = \\\n",
    "            self.bert(input_ids, token_type_id, input_mask)\n",
    "        seq = self.dropout(sequence_output)\n",
    "        seq = self.reshape(seq, self.shape)\n",
    "        logits = self.dense_1(seq)\n",
    "        logits = self.cast(logits, self.dtype)\n",
    "        \n",
    "        if self.use_crf:\n",
    "            return_value = self.reshape(logits, self.origin_shape)\n",
    "            loss = self.loss(return_value, label_ids)\n",
    "        else:\n",
    "            return_value = self.log_softmax(logits)\n",
    "            loss = self.loss(return_value, label_ids, self.num_labels)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 6. 加载词汇-id映射表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 0, 'S_address': 1, 'B_address': 2, 'M_address': 3, 'E_address': 4, 'S_book': 5, 'B_book': 6, 'M_book': 7, 'E_book': 8, 'S_company': 9, 'B_company': 10, 'M_company': 11, 'E_company': 12, 'S_game': 13, 'B_game': 14, 'M_game': 15, 'E_game': 16, 'S_government': 17, 'B_government': 18, 'M_government': 19, 'E_government': 20, 'S_movie': 21, 'B_movie': 22, 'M_movie': 23, 'E_movie': 24, 'S_name': 25, 'B_name': 26, 'M_name': 27, 'E_name': 28, 'S_organization': 29, 'B_organization': 30, 'M_organization': 31, 'E_organization': 32, 'S_position': 33, 'B_position': 34, 'M_position': 35, 'E_position': 36, 'S_scene': 37, 'B_scene': 38, 'M_scene': 39, 'E_scene': 40}\n"
     ]
    }
   ],
   "source": [
    "tag_to_index = json.loads(open(cfg.label2id_file).read())\n",
    "\n",
    "if cfg.use_crf:\n",
    "    print(tag_to_index)\n",
    "    max_val = len(tag_to_index)\n",
    "    tag_to_index[\"<START>\"] = max_val\n",
    "    tag_to_index[\"<STOP>\"] = max_val + 1\n",
    "    number_labels = len(tag_to_index)\n",
    "else:\n",
    "    number_labels = cfg.num_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 7. 定义训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    '''\n",
    "    finetune function\n",
    "    '''\n",
    "    # BertNER train for sequence labeling\n",
    "\n",
    "    netwithloss = BertNER(bert_net_cfg, cfg.batch_size, True, num_labels=number_labels,\n",
    "                          use_crf=cfg.use_crf,\n",
    "                          tag_to_index=tag_to_index, dropout_prob=0.1)\n",
    "\n",
    "    dataset = get_dataset(data_file=cfg.train_file, schema_file=cfg.schema_file, batch_size=cfg.batch_size)\n",
    "    steps_per_epoch = dataset.get_dataset_size()\n",
    "    print('steps_per_epoch:',steps_per_epoch)\n",
    "\n",
    "    # optimizer\n",
    "    steps_per_epoch = dataset.get_dataset_size()\n",
    "    lr_schedule = BertLearningRate(learning_rate=optimizer_cfg.AdamWeightDecay.learning_rate,\n",
    "                                   end_learning_rate=optimizer_cfg.AdamWeightDecay.end_learning_rate,\n",
    "                                   warmup_steps=int(steps_per_epoch * cfg.epoch_num * 0.1),\n",
    "                                   decay_steps=steps_per_epoch * cfg.epoch_num,\n",
    "                                   power=optimizer_cfg.AdamWeightDecay.power)\n",
    "    params = netwithloss.trainable_params()\n",
    "    decay_params = list(filter(optimizer_cfg.AdamWeightDecay.decay_filter, params))\n",
    "    other_params = list(filter(lambda x: not optimizer_cfg.AdamWeightDecay.decay_filter(x), params))\n",
    "    group_params = [{'params': decay_params, 'weight_decay': optimizer_cfg.AdamWeightDecay.weight_decay},\n",
    "                    {'params': other_params, 'weight_decay': 0.0}]\n",
    "    optimizer = AdamWeightDecay(group_params, lr_schedule, eps=optimizer_cfg.AdamWeightDecay.eps)\n",
    "        \n",
    "    # load checkpoint into network\n",
    "    ckpt_config = CheckpointConfig(save_checkpoint_steps=steps_per_epoch, keep_checkpoint_max=1)\n",
    "    ckpoint_cb = ModelCheckpoint(prefix=cfg.ckpt_prefix, directory=cfg.ckpt_dir, config=ckpt_config)\n",
    "    param_dict = load_checkpoint(cfg.pre_training_ckpt)\n",
    "    load_param_into_net(netwithloss, param_dict)\n",
    "\n",
    "    update_cell = DynamicLossScaleUpdateCell(loss_scale_value=2**32, scale_factor=2, scale_window=1000)\n",
    "    netwithgrads = BertFinetuneCell(netwithloss, optimizer=optimizer, scale_update_cell=update_cell)\n",
    "    model = Model(netwithgrads)\n",
    "    callbacks = [TimeMonitor(dataset.get_dataset_size()), LossMonitor(), ckpoint_cb]\n",
    "    model.train(cfg.epoch_num, dataset, callbacks=callbacks, dataset_sink_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 8. 启动训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 9. 加载离线模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "netwithloss = BertNER(bert_net_cfg, 1, False, num_labels=number_labels,\n",
    "                     use_crf=cfg.use_crf,\n",
    "                     tag_to_index=tag_to_index)\n",
    "\n",
    "netwithloss.set_train(False)\n",
    "param_dict = load_checkpoint(cfg.finetune_ckpt)\n",
    "load_param_into_net(netwithloss, param_dict)\n",
    "model = Model(netwithloss)\n",
    "\n",
    "tokenizer_ = tokenization.FullTokenizer(vocab_file=cfg.vocab_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 10. 定义测试集评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def eval():\n",
    "    '''\n",
    "    evaluation function\n",
    "    '''\n",
    "\n",
    "    dataset = get_dataset(cfg.eval_file, cfg.schema_file, 1)\n",
    "    columns_list = [\"input_ids\", \"input_mask\", \"segment_ids\", \"label_ids\"]\n",
    "\n",
    "    y_true, y_pred = [], []\n",
    "    for data in dataset.create_dict_iterator():\n",
    "        input_data = []\n",
    "        for i in columns_list:\n",
    "            input_data.append(Tensor(data[i]))\n",
    "        input_ids, input_mask, token_type_id, label_ids = input_data\n",
    "        logits = model.predict(input_ids, input_mask, token_type_id, label_ids)\n",
    "\n",
    "        if cfg.use_crf:\n",
    "            backpointers, best_tag_id = logits\n",
    "            best_path = postprocess(backpointers, best_tag_id)\n",
    "            logit_ids = []\n",
    "            for ele in best_path:\n",
    "                logit_ids.append(ele)\n",
    "        else:\n",
    "            logits = logits.asnumpy()\n",
    "            logit_ids = np.argmax(logits, axis=-1)\n",
    "\n",
    "        for ids in label_ids.asnumpy():\n",
    "            y_true.extend(ids)\n",
    "        for ids in logit_ids:\n",
    "            y_pred.extend(ids)\n",
    "\n",
    "    print(classification_report(y_true, y_pred, labels=range(1, 41), target_names=list(tag_to_index.keys())[1:41]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 11. 启动测试集评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "     S_address       0.00      0.00      0.00         0\n",
      "     B_address       0.72      0.80      0.76       373\n",
      "     M_address       0.80      0.84      0.82       956\n",
      "     E_address       0.71      0.73      0.72       373\n",
      "        S_book       0.00      0.00      0.00         0\n",
      "        B_book       0.84      0.84      0.84       154\n",
      "        M_book       0.93      0.85      0.89       723\n",
      "        E_book       0.83      0.81      0.82       154\n",
      "     S_company       0.00      0.00      0.00         0\n",
      "     B_company       0.88      0.89      0.88       378\n",
      "     M_company       0.81      0.85      0.83       937\n",
      "     E_company       0.82      0.84      0.83       378\n",
      "        S_game       0.00      0.00      0.00         0\n",
      "        B_game       0.84      0.93      0.88       295\n",
      "        M_game       0.85      0.94      0.90      1067\n",
      "        E_game       0.83      0.94      0.88       295\n",
      "  S_government       0.00      0.00      0.00         0\n",
      "  B_government       0.83      0.89      0.86       247\n",
      "  M_government       0.83      0.91      0.87       821\n",
      "  E_government       0.84      0.90      0.87       247\n",
      "       S_movie       0.00      0.00      0.00         0\n",
      "       B_movie       0.90      0.84      0.87       151\n",
      "       M_movie       0.93      0.90      0.91       741\n",
      "       E_movie       0.90      0.83      0.86       151\n",
      "        S_name       0.00      0.00      0.00         0\n",
      "        B_name       0.89      0.92      0.90       465\n",
      "        M_name       0.86      0.89      0.88       556\n",
      "        E_name       0.85      0.91      0.88       465\n",
      "S_organization       0.00      0.00      0.00         0\n",
      "B_organization       0.84      0.84      0.84       367\n",
      "M_organization       0.79      0.70      0.74       720\n",
      "E_organization       0.81      0.83      0.82       367\n",
      "    S_position       0.00      0.00      0.00         0\n",
      "    B_position       0.79      0.84      0.81       433\n",
      "    M_position       0.74      0.77      0.76       335\n",
      "    E_position       0.88      0.94      0.91       433\n",
      "       S_scene       0.00      0.00      0.00         0\n",
      "       B_scene       0.77      0.76      0.76       209\n",
      "       M_scene       0.83      0.81      0.82       513\n",
      "       E_scene       0.76      0.78      0.77       209\n",
      "\n",
      "     micro avg       0.83      0.86      0.85     13513\n",
      "     macro avg       0.62      0.64      0.63     13513\n",
      "  weighted avg       0.83      0.86      0.85     13513\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 12. 定义在线推理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def inference(text):\n",
    "    \"\"\"\n",
    "    online inference\n",
    "    \"\"\"\n",
    "    feature = process_one_example_p(tokenizer_, cfg.vocab_file, text, max_seq_len=bert_net_cfg.seq_length)\n",
    "    input_ids, input_mask, token_type_id = feature\n",
    "    input_ids = Tensor(np.array(input_ids), mstype.int32)\n",
    "    input_mask = Tensor(np.array(input_mask), mstype.int32)\n",
    "    token_type_id = Tensor(np.array(token_type_id), mstype.int32)\n",
    "    if cfg.use_crf:\n",
    "        backpointers, best_tag_id = model.predict(input_ids, input_mask, token_type_id, Tensor(1))\n",
    "        best_path = postprocess(backpointers, best_tag_id)\n",
    "        logits = []\n",
    "        for ele in best_path:\n",
    "            logits.extend(ele)\n",
    "        ids = logits\n",
    "    else:\n",
    "        logits = model.predict(input_ids, input_mask, token_type_id, Tensor(1))\n",
    "        ids = logits.asnumpy()\n",
    "        ids = np.argmax(ids, axis=-1)\n",
    "        ids = list(ids)\n",
    "        \n",
    "    res = label_generation(text=text, probs=ids, tag_to_index=tag_to_index)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 13. 在线推理测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': {'温格': [[0, 1]]},\n",
       " 'organization': {'曼联': [[23, 24]], '枪手': [[27, 28]]}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference(\"温格的球队终于又踢了一场经典的比赛，2比1战胜曼联之后枪手仍然留在了夺冠集团之内，\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'address': {'文汇路': [[6, 8]]}, 'government': {'教委': [[38, 39]]}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference(\"郑阿姨就赶到文汇路排队拿钱，希望能将缴纳的一万余元学费拿回来，顺便找校方或者教委要个说法。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore-python3.7-aarch64",
   "language": "python",
   "name": "mindspore-python3.7-aarch64"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}